\subsection{Header files}
\begin{lstlisting}
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include "mpi.h"
\end{lstlisting}

\subsection{Main}
\subsubsection{Dichiarazione delle risorse}
\begin{lstlisting}
int main(int argc, char *argv[]) {
  float **A;
  float **B;
  float **C;

  int n;
  int pid, nproc;

  MPI_Init(&argc, &argv);

  MPI_Status status;
  MPI_Request request;

  MPI_Comm_rank(MPI_COMM_WORLD, &pid);
  MPI_Comm_size(MPI_COMM_WORLD, &nproc);

  A = readMatrix("<path>/matrix1.txt", &n);
  B = readMatrix("<path>/matrix2.txt", &n);

  int m = sqrt(nproc); // suppongo che nproc sia un quadrato perfetto la cui radice divide n

  MPI_Comm grid;

  create_grid_communicator(m, &grid);

  int i, j;
  int grid_rank;

  MPI_Comm_rank(grid, &grid_rank);

  int coords[2];
  MPI_Cart_coords(grid, grid_rank, 2, coords);

  int blockSize = n / m;

  double startTime, endTime, time, timetot;
\end{lstlisting}
Una descrizione delle variabili:
\begin{itemize}
    \item A, B, C sono le matrici coinvolte nel calcolo.
    \item n é la dimensione delle suddette matrici, che abbiamo supposto quadrate.
    \item pid é l'identificativo di un processo all'interno del communicator MPI\_COMM\_WORLD.
    \item nproc é il numero di processi lanciati.
    \item L'invocazione di MPI\_Init sancisce l'inizio delle comunicazioni tra processi.
    \item status, di tipo MPI\_Status, rappresenta lo stato di un'operazione di MPI\_Recv.
    \item request, di tipo MPI\_Request, é il corrispettivo di status per i messaggi non bloccanti.
    \item L'invocazione di MPI\_Comm\_rank assegna ad ogni processo il suo pid, mentre MPI\_Comm\_size rende noto ad ogni processo il numero di processi nel communicator.
    \item La funzione readMatrix, dato il path di un file, legge la dimensione della matrice in esso, aggiorna la variabile n nel main, restituisce una matrice allocata.
    \item m é la dimensione della griglia di processi che verrá creata.
    \item grid é un MPI\_Comm che viene inizializzato dalla funzione create\_grid\_communicator.
    \item i e j sono indici che verranno utilizzati in seguito.
    \item grid\_rank é l'identificativo di un processo nella griglia.
    \item L'invocazione di MPI\_Comm\_rank assegna a ciascun processo il suo grid\_rank.
    \item coords é il vettore delle coordinate di ciascun processo all'interno della griglia.
    \item La chiamata a MPI\_Cart\_coords assegnerá tali coordinate.
    \item blockSize é la dimensione delle sottomatrici delle matrici A, B, C.
    \item startTime, endTime, time, timetot sono le variabili utilizzate per la registrazione del tempo di esecuzione. Esse sono, rispettivamente, il valore dell'istante iniziale, dell'istante finale, l'intervallo effettivo tra i due istanti precedenti, il valore massimo tra i valori di time tra tutti i processi. 
\end{itemize}

\subsubsection{Distribuzione dei dati}
\begin{lstlisting}
  C = allocMatrix(n, n);
  float **A_local_temp = allocMatrix(blockSize, blockSize);
  float **A_local = allocMatrix(blockSize, blockSize);
  float **B_local = allocMatrix(blockSize, blockSize);
  float **C_local = allocMatrix(blockSize, blockSize); // inizializzare a 0
  C_local = initMatrix(C_local, blockSize, blockSize);
  float **C_local_temp = allocMatrix(blockSize, blockSize);

  if (coords[0] == 0 && coords[1] == 0) {

    for (i = 0; i < m; i++) {
      for (j = 0; j < m; j++) {

        if (i == 0 && j == 0) {
          // riempi A_local da A
          A_local = createSubmatrix(A, blockSize, 0, 0);
          B_local = createSubmatrix(B, blockSize, 0, 0);
        } else {
          // crea Datatype subarray secondo l offset e dividi la matrice in
          // sottomatrici.

          MPI_Datatype blockMatrix;

          int sizes[2] = {n, n};
          int subsizes[2] = {blockSize, blockSize};
          int starts[2] = {i * blockSize, j * blockSize};

          MPI_Type_create_subarray(2, sizes, subsizes, starts, MPI_ORDER_C,
                                   MPI_FLOAT, &blockMatrix);
          MPI_Type_commit(&blockMatrix);
          MPI_Send(&(A[0][0]), 1, blockMatrix, (i * m) + j, (i * m) + j + 20,
                   grid); // dividamo e mandiamo la matrice A
          MPI_Send(&(B[0][0]), 1, blockMatrix, (i * m) + j, (i * m) + j + 900,
                   grid); // dividiamo e mandiamo la matrice B
        }
      }
    }
  } else {
    MPI_Recv(&(A_local[0][0]), blockSize * blockSize, MPI_FLOAT, 0,
             grid_rank + 20, grid,
             &status); // riceviamo la sottomatrice di A in A_local
    MPI_Recv(&(B_local[0][0]), blockSize * blockSize, MPI_FLOAT, 0,
             grid_rank + 900, grid,
             &status); // riceviamo la sottomatrice di B in B_local
  }
\end{lstlisting}
Notiamo che l'implementazione della strategia scelta imponga a ciascun processo di allocare due slot di sottomatrici di A: A\_local (la sottomatrice di competenza di un processo) e A\_local\_temp (la sottomatrice ricevuta da un altro processo della stessa riga, come si vedrá nella prossima sottosezione).
Il processo 0 calcola la posizione della sottomatrice da inviare a ciascun processo, crea un MPI\_Datatype blockMatrix (l'effettiva creazione di esso é effettuata mediante l'invocazione di MPI\_Type\_commit) ed invia tale sottomatrice al suo processo.
Gli altri processi ricevono le proprie sottomatrici dal processo 0.

\subsubsection{Algoritmo BMR}
Il seguente codice é non altro che l'implementazione in MPI dello pseudocodice descritto nella sezione introduttiva. Nota: é necessario che l'invio della matrice B\_local sia non bloccante (MPI\_Irecv). Altrimenti, i processi si bloccherebbero in attesa di una ricezione che non avverrá mai, poiché sono tutti a dover ricevere. Invece, per assicurarci che le operazioni all'iterazione successiva siano svolte con la matrice B\_local corretta, la ricezione di essa é bloccante (MPI\_Recv).
\begin{lstlisting}
  double startTime = MPI_Wtime();

  for (i = 0; i < m;
       i++) { // l algoritmo di FOX finisce dopo m passi (scorro le diagonali)

    // mandiamo il blocco di A_locale ai processori sulla stessa riga

    if (coords[0] == mod(coords[1] - i, m)) { // se sono sulla diagonale
      for (j = 0; j < m; j++) {
        int dest = coords[0] * m + j;
        if (dest != grid_rank) {
          MPI_Send(&(A_local[0][0]), blockSize * blockSize, MPI_FLOAT, dest,
                   dest + 80 * (i + 1),
                   grid); // manda A_local a tutti quelli della mia riga
        }
      }
    } else {
      int sorgente = (coords[0] * m) + (mod(coords[0] + i, m));
      MPI_Recv(&(A_local_temp[0][0]), blockSize * blockSize, MPI_FLOAT,
               sorgente, grid_rank + 80 * (i + 1), grid,
               &status); // ricevi sulla tua riga
    }

    if (coords[0] == mod(coords[1] - i, m)) { // se sono sulla diagonale
      C_local_temp = matrixMultiplication(
          A_local, B_local, blockSize); // ho quello che mi serve in A_local
    } else {
      C_local_temp = matrixMultiplication(
          A_local_temp, B_local, blockSize); // altrimenti lo ho in A_local_temp
    }

    C_local =
        addMatrix(C_local, C_local_temp,
                  blockSize); // addizioniamo il risultato a quelli precendenti

    // mandiamo il blocco di B_locale ai processori sulla stessa colonna ma
    // sulla riga precedente

    MPI_Isend(&(B_local[0][0]), blockSize * blockSize, MPI_FLOAT,
             mod(coords[0] - 1, m) * m + coords[1],
             mod(coords[0] - 1, m) * m + coords[1] + 100, grid, &request);
    MPI_Recv(&(B_local[0][0]), blockSize * blockSize, MPI_FLOAT,
             mod(coords[0] + 1, m) * m + coords[1], grid_rank + 100, grid,
             &status);
  }

  double endTime = MPI_Wtime();
\end{lstlisting}

\subsubsection{Ricostruzione della matrice risultante C}
\begin{lstlisting}
  if (coords[0] == 0 && coords[1] == 0) {
    MPI_Datatype blockMatrix;

    for (i = 0; i < m; i++) {
      for (j = 0; j < m; j++) {
        if (i != 0 || j != 0) {
          int sizes[2] = {n, n};
          int subsizes[2] = {blockSize, blockSize};
          int starts[2] = {i * blockSize, j * blockSize};

          MPI_Type_create_subarray(2, sizes, subsizes, starts, MPI_ORDER_C, MPI_FLOAT, &blockMatrix);
          MPI_Type_commit(&blockMatrix);
          MPI_Recv(&(C[0][0]), 1, blockMatrix, (i * m) + j, (i * m) + j + 500, grid, &status);

        } else {
          int k, h;
          for (k = 0; k < blockSize; k++) {
            for (h = 0; h < blockSize; h++) {
              C[k][h] = C_local[k][h];
            }
          }
        }
      }
    }

  } else {
    MPI_Send(&(C_local[0][0]), blockSize * blockSize, MPI_FLOAT, 0,
             grid_rank + 500, grid);
  }
\end{lstlisting}

\subsubsection{Calcolo del tempo d'esecuzione}
\begin{lstlisting}
  time = endTime - startTime;
  timetot;

  MPI_Reduce(&time, &timetot, 1, MPI_DOUBLE, MPI_MAX, 0, grid);

  // scriviamo i risultati su file
  if (coords[0] == 0 && coords[1] == 0) {
    printResultsOnFile("risultati.txt", timetot, nproc, m, n);
  }
\end{lstlisting}

\subsubsection{Chiusura del programma}
\begin{lstlisting}
  MPI_Finalize();
  return 0;
\end{lstlisting}

\subsection{Funzioni accessorie}
Di seguito, una carrellata di funzioni richiamate dal Main.
\subsubsection{Lettura da file di una matrice}
\begin{lstlisting}
float **readMatrix(char *s, int *n) {
  FILE *fp;
  fp = fopen(s, "r");

  fscanf(fp, "%d", n);

  int i, j;

  float **M = allocMatrix((*n), (*n));

  for (i = 0; i < (*n); i++) {
    for (j = 0; j < (*n); j++) {
      fscanf(fp, "%f", &(M[i][j]));
    }
  }

  fclose(fp);

  return M;
}
\end{lstlisting}

\subsubsection{Creazione di un communicator di griglia}
\begin{lstlisting}
void create_grid_communicator(int m, MPI_Comm *grid) {

  int dim = 2;
  int dims[2];
  dims[0] = m;
  dims[1] = m;
  int wrap[2];
  wrap[0] = 0;
  wrap[1] = 0;
  int optimize = 0;

  if (MPI_SUCCESS !=
      MPI_Cart_create(MPI_COMM_WORLD, dim, dims, wrap, optimize, grid)) {
    gestisciErrore(ERRORE_CREAZIONE_GRIGLIA);
  }
}
\end{lstlisting}
dim indica il numero di dimensione della griglia.
dims é il vettore contenente la lunghezza di ciascuna dimensione.
wrap é il vettore contenente un valore booleano: se 1, la dimensione é periodica, se 0 no.
optimize é anche esso un valore booleano, che, se 1, comanda la riorganizzazione degli identificativi dei processi della griglia.
\subsubsection{Overriding dell'operazione modulo}
L'overriding é stato necessario poiché nell'algoritmo implementato era richiesta l'operazione di modulo nella sua definizione algebrica, non presente nel C standard.
\begin{lstlisting}
int mod(int a, int m) {
  int ret;
  if (a < 0) {
    while (a < 0) {
      a += m;
    }
    return a;
  } else {
    ret = a % m;
    return ret;
  }
}
\end{lstlisting}
\subsubsection{Allocazione di una matrice in memoria contigua}
\begin{lstlisting}
float **allocMatrix(int m, int n) {
  float **M;

  M = (float **)malloc(m * sizeof(float *));
  if (M == NULL) {
    gestisciErrore(ERRORE_ALLOCAZIONE_MEMORIA);
    free(M);
    return NULL;
  }

  M[0] = (float *)malloc(m * n * sizeof(float));
  if (M[0] == NULL) {
    gestisciErrore(ERRORE_ALLOCAZIONE_MEMORIA);
    free(M[0]);
    free(M);
    return NULL;
  }

  int i;
  for (i = 1; i < m; i++) {
    M[i] = M[i - 1] + n;
  }

  return M;
}
\end{lstlisting}
\subsubsection{Inizializzazione di una matrice}
\begin{lstlisting}
float **initMatrix(float **A, int m, int n) {
  int i, j;
  for (i = 0; i < m; i++) {
    for (j = 0; j < n; j++) {
      A[i][j] = 0;
    }
  }
  return A;
}
\end{lstlisting}

\subsubsection{Liberazione della memoria occupata da una matrice contigua}
\begin{lstlisting}
void freeMatrix(float **M) {
  free(M[0]);
  free(M);
}
\end{lstlisting}
\subsubsection{Addizione tra matrici}
\begin{lstlisting}
float **addMatrix(float **A, float **B, int size) {
  int i, j;
  for (i = 0; i < size; i++) {
    for (j = 0; j < size; j++) {
      A[i][j] += B[i][j];
    }
  }
  return A;
}
\end{lstlisting}

\subsubsection{Prodotto scalare standard}
\begin{lstlisting}
float stdScalarProduct(float **A, float **B, const int i, const int j,
                       int size) { // const per enfatizzare il loro non //essere variabili in questo blocco.
  int h, ret;
  ret = 0;
  for (h = 0; h < size; h++) {
    ret += A[i][h] * B[h][j];
  }
  return ret;
}
\end{lstlisting}
\subsubsection{Prodotto matrice-matrice}
\begin{lstlisting}
float **matrixMultiplication(float **A, float **B, int size) {
  float **C = allocMatrix(size, size);
  int i, j, k;
  for (i = 0; i < size; i++) {
    for (j = 0; j < size; j++) {
      C[i][j] = stdScalarProduct(A, B, i, j, size);
    }
  }
  return C;
}
\end{lstlisting}
\subsubsection{Creazione di un sottomatrice}
\begin{lstlisting}
float **createSubmatrix(float **A, int size, int offsetRow, int offsetCol) {
  float **R = allocMatrix(size, size);
  int i, j;
  for (i = offsetRow; i < size; i++) {
    for (j = offsetCol; j < size; j++) {
      R[i - offsetRow][j - offsetCol] = A[i][j];
    }
  }
  return R;
}
\end{lstlisting}
\subsubsection{Stampa dei risultati su file}
\begin{lstlisting}
void printResultsOnFile(char *path, double t, int nproc, int m, int n) {
  FILE *fp;
  fp = fopen(path, "a");

  if (fp == NULL) {
    gestisciErrore(ERRORE_APERTURA_FILE);
  }

  fprintf(fp, "dimensione matrice--->(%dx%d) \n", n, n);
  fprintf(fp, "numero di processori--->%d  matrice(%dx%d) \n", nproc, m, m);
  fprintf(fp, "tempo di esecuzione--->%e \n\n", t);

  fclose(fp);
}
\end{lstlisting}
\subsubsection{Error Handler}
\begin{lstlisting}
void gestisciErrore(int code) {
  switch (code) {
  case ERRORE_ALLOCAZIONE_MEMORIA:
    printf("Errore nell allocazione della matrice in memoria\n");
    exit(-1);
  case ERRORE_CREAZIONE_GRIGLIA:
    printf("Errore nella creazione del communicator griglia\n");
    exit(-1);
  case ERRORE_APERTURA_FILE:
    printf("Errore nell apertura del file\n");
    exit(-1);
  default:
    exit(-1);
  }
}
\end{lstlisting}