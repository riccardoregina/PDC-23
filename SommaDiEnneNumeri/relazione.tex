\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[export]{adjustbox}
\usepackage{listings}
\usepackage{color}
\usepackage{tabularray}
\usepackage{hyperref}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
frame=tb,
language=C,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
basicstyle={\small\ttfamily},
numbers=none,
numberstyle=\tiny\color{gray},
keywordstyle=\color{blue},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=true,
breakatwhitespace=true,
tabsize=3
}

\title{Algoritmo parallelo per la somma di N numeri}
\author{Puggioni Riccardo, Regina Riccardo, Trotti Francesco }
\date{October 2023}

\begin{document}

\maketitle

\newpage
\tableofcontents

\newpage
\section{Introduzione al problema}
\subsection{Somma di N numeri}
    L'algoritmo per la somma di N numeri fa parte di quei problemi
    il cui approccio alla risoluzione è detto di "riduzione".
    Parallelizzare questo tipo di algoritmi può essere particolarmente insidioso:
    quando si sommano N numeri quello che si fa è effettuare un'assegnazione ed eseguire N-1
    operazioni di somma; il nostro obiettivo è cercare di parallelizzare le operazioni di somma eseguendole su più processori diminuendo in questo modo i passi temporali necessari alla risoluzione del problema.

    Ciò che però ci preme è non solo di risolvere il problema utilizzando un algoritmo parallelo, ma cercare di sfruttare al meglio il nostro ambiente di calcolo.
\subsection{Approccio alla risoluzione del problema dato}

    Andiamo dunque ad implementare la strategia di parallelizzazione in linguaggio C utilizzando una libreria per il "message passing" in un sistema MIMD: la libreria MPI.
    \\
    L'idea principale per la risoluzione del problema è di dividere l' array fra i vari processori.
    Fatto questo, ogni processore dovrà calcolare la propria somma parziale, ovvero la somma degli elementi del proprio sottoarray.
    Si dovranno poi ricostruire le varie somme parziali in maniera opportuna per ottenere la soluzione finale.\\
    \begin{figure}[!htbp]
        \includegraphics[width=0.3\linewidth,center]{divisione_array.drawio.png}
        \caption{Divisione dell array in parti}
        \label{fig:enter-label}
    \end{figure}  
    
    Schematizzando:
    \begin{enumerate}
        \item dividiamo l'array in più parti e assegniamo ogni parte ad un processore.
        \item ogni processore esegue la propria somma parziale.
        \item si ricombinano le soluzioni parziali e si ottiene la soluzione finale.
    \end{enumerate}

    In questo modo possiamo così eseguire più somme concorrentemente, e il nostro algoritmo sarà di conseguenza più rapido.\\
    Una prima idea sarebbe dunque quella di prendere la dimensione dell'array e dividerla per il numero dei processori utilizzati: in questo modo otteniamo (all'incirca) il numero di elementi che ogni sotto-array dovrà contenere (a meno dei resti della divisione che si dovranno poi gestire nell'implementazione).

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.6\linewidth]{somme_parziali.drawio (1).png}
        \caption{Ogni processore esegue la sua somma}
        \label{fig:enter-label}
    \end{figure}

    Una volta eseguita la distribuzione dei dati fra i vari processori passiamo all' algoritmo vero e proprio: ogni processore non deve far altro che sommare gli elementi del proprio sotto-array producendo le somme parziali.\\
    

\subsection{Strategie di comunicazione}
 Bisogna ora capire il modo in cui queste somme devono essere ricombinate, a tal scopo analizziamo tre possibili strategie:
\subsubsection{Strategia I}
    La prima strategia è la più intuitiva: consiste nel prendere le varie somme parziali ed inviarle tutte ad un processore predefinito, che eseguirà la somma di tali somme parziali e, ricombinandole, otterrà la soluzione finale al problema.
    Tuttavia seppur questa implementazione sia molto semplice (e funzionante in ogni caso), non rappresenta sempre la scelta più performante: una volta che inviamo al processore predefinito tutte le somme parziali, questo eseguirà la somme rimanenti in maniera sequenziale, mentre gli altri processori non saranno utilizzati affatto. La somma finale sarà presente dunque soltanto nella memoria del processore scelto per il calcolo finale.\\
    Come si evince anche dalla figura, ci saranno delle somme che potevano essere parallelizzate ulteriormente...(in date condizioni). 

    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{strategia_I.drawio.png}
        \caption{Ognuno invia ad un processore}
        \label{fig:enter-label}
    \end{figure}
\subsubsection{Strategia II}
    I processori si dividono in coppie: uno dei due invia all'altro la propria somma parziale e l'altro, ricevendo i dati, aggiorna la propria somma.
    Alla fine, l'ultima coppia aggiornerà l'ultima somma parziale ottenendo il risultato finale.
    L'ultimo processore a ricevere conterrà la somma totale.\\
    Tuttavia ci accorgiamo subito che, in una prima analisi preliminare, l'implementazione di questa strategia fa sicuramente sì che si risparmino passi temporali, ma può essere applicata soltanto se il numero dei processori in gioco è una potenza di due. Infatti, non potrei dividere i processori in coppie se questa condizione non fosse soddisfatta.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{strategia_II.drawio.png}
    \caption{In coppie (uno manda e uno riceve)}
    \label{fig:enter-label}
\end{figure}
\subsubsection{Strategia III}
    La terza strategia è simile alla seconda: divido sempre i processori in coppia.
    Tuttavia questa volta ogni coppia di processori si invia a vicenda la propria somma parziale: entrambi aggiornano la loro somma. Alla fine tutti i processori avranno il risultato finale in memoria. Tuttavia i passi temporali che risparmiamo sono gli stessi della strategia due, la principale differenza è infatti nel solo fatto che tutti i processori alla fine avranno la somma totale.
    \begin{figure}[!h tbp]
        \centering
        \includegraphics[width=1\linewidth]{strategia_III.drawio.png}
        \caption{In coppie (entrambi mandano e ricevono)}
        \label{fig:enter-label}
    \end{figure}
\clearpage
\section{Implementazione dell'algoritmo}
\subsection{Header file richiamati}
\begin{lstlisting}
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include "mpi.h"
\end{lstlisting}

\subsection{Implementazione}
\begin{lstlisting}


#define BAD_strategia_VALUE 10
#define BAD_ARRAY_SIZE 20
#define ERRORE_ALLOCAZIONE_MEMORIA 30
void gestisciErrore(int error_code);
int isPowerOf2(int x);

int main(int argc,char *argv[])
{
    int pid, n_processi; //pid è il process id, n_processi è il numero di processi
    int strategia; //la strategia specificata dall'utente
    int *array; //array che conterrà i numeri letti da file
    int dim; // dimensione di array
    int *array_locale, start, dim_locale; //il sottoarray di competenza di un processo
    //start è l'indirizzo del primo elemento di un sottoarray
    //dim_locale è la dimensione del sottoarray
    int rest; //gli elementi in esubero da ridistribuire equamente ai processi
    int tag; //tag è l'identificativo di una comunicazione tra due processi
    double tempo_tot; //tempo totale serve al processo 0 per calcolare il massimo fra tutti i tempi di esecuzione
    double t_0; // serve ad ogni processo per prendere il tempo iniziale
    double t_1; // serve ad ogni processo per prendere il tempo finale
    double time; // serve ad ogni processo per calcolare t_1 - t_0
    MPI_Status status; //indica lo stato di una comunicazione

\end{lstlisting}

\subsection{Scelta strategia}

\begin{lstlisting}
    MPI_Init(&argc, &argv); //Inizio delle comunicazioni

    strategia = atoi(argv[1]);
    if(pid==0)printf(" before--->%d",strategia);
    if (strategia != 1 && strategia != 2 && strategia != 3) {
        gestisciErrore(BAD_STRATEGY_VALUE);
        strategia = 1;
        //continuo lo stesso con la strategia 1
    }


    MPI_Comm_rank(MPI_COMM_WORLD, &pid); //ogni processo inserisce il proprio identificativo nella variabile pid. 
    MPI_Comm_size(MPI_COMM_WORLD, &n_processi); //Ogni processo viene a conoscenza del numero di processi.

    //Decido la strategia
    int potenzaDi2 = isPowerOf2(n_processi);
    //se strategia = 2,3 AND n_processi non è potenza di 2 ALLORA procedo con la strategia 1. 
    if (potenzaDi2 == 0 && (strategia == 2 || strategia == 3)) {
        printf("Strategie 1, 2 non applicabili: procedo con la strategia 1\n");
        strategia = 1;
    }
\end{lstlisting}

\subsection{Distribuzione}

\begin{lstlisting}

    if (pid==0) {
        //leggi i dati dall input (n e x)
        FILE *fp;
        fp = fopen("/homes/DMA/PDC/2024/<LOGIN>/somma/somma.txt","r"); //e' richiesto il PATH intero

        fscanf(fp,"%d ",&dim);
        if (dim < n_processi || dim < 0) {
            gestisciErrore(BAD_ARRAY_SIZE);
            exit(1);
        }

        array = malloc(dim*sizeof(int));
        if (array == NULL) {
            gestisciErrore(ERRORE_ALLOCAZIONE_MEMORIA);
            exit(1);
        }
        
        int i;
        for (i = 0; i < dim; i++){
            fscanf(fp,"%d ",&array[i]);
        }
        
        fclose(fp);
    }

    MPI_Bcast(&dim, 1, MPI_INT, 0, MPI_COMM_WORLD); //ogni processo ottiene la dimensione dell array di input (i processi si sincronizzano).

    dim_locale = dim / n_processi; //dividiamo la dimensione dell array per il numero di processi 
    rest = dim % n_processi;     //e gestiamo il resto in modo da ottenere la dimensione
    if (pid < rest) {           // del sottoarray per ogni processo.
        dim_locale++;
    }

    int tmp;
    if (pid==0) {
        array_locale = array;
        tmp = dim_locale;
        start = 0;
        int i;
	    for (i = 1; i < n_processi; i++){    //spezziamo l array in piu parti e mandiamo
            start = start + tmp;              //ad ogni processo la sua parte.
            tag = 22 + i;
            if (i == rest) {	
                tmp--;
		    }
            MPI_Send(&array[start], tmp, MPI_INT, i, tag, MPI_COMM_WORLD);
        }
    }
    else
    {
        tag = 22 + pid;
        array_locale = malloc(dim_locale*sizeof(int));
	    MPI_Recv(array_locale, dim_locale, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);  
        //i processi ricevono la propria parte.
    }
\end{lstlisting}

\subsection{Misura istante iniziale}

\begin{lstlisting}

    MPI_Barrier(MPI_COMM_WORLD); //sincronizzo i processi in modo che tutti partano dallo stesso istante.
    t_0=MPI_Wtime(); // tutti i processi prendono l'istante iniziale t_0.
\end{lstlisting}

\subsection{Somma parallelizzata}

\begin{lstlisting}

    //tutti i processori eseguono la propria somma parziale:
    int somma_parziale;
    int i;
    int somma=0;
	for (i = 0; i<dim_locale; i++) {
       	somma = somma + array_locale[i];
	}
    printf("sono %d e la mia somma parziale--->%d\n",pid,somma);
\end{lstlisting}

\subsection{Blocco if strategia}

\begin{lstlisting}
    
    //Strategia di comunicazione
    if(pid==0)printf("eseguo startegia:%d\n",strategia);
    if (strategia == 1) {
        if (pid == 0){
            for (i = 1; i < n_processi; i++){
                tag = 69 + i;
                MPI_Recv(&somma_parziale, 1, MPI_INT, i, tag, MPI_COMM_WORLD, &status);
                somma = somma + somma_parziale;
            }
        }
        else
        {
            tag = 69 + pid;
            MPI_Send(&somma, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);
        }
    }
    else if (strategia == 2) {
        for (i = 0; i < log2(n_processi); i++) {
            if (pid % (int)pow(2,i) == 0) {
                if (pid % (int)pow(2,i+1) == 0) {
                    tag = pid;
                    MPI_Recv(&somma_parziale, 1, MPI_INT, pid+pow(2,i), tag, MPI_COMM_WORLD, &status);
		    somma = somma + somma_parziale;
                } 
                else
                {
                    tag = abs(pow(2,i) - pid);
                    MPI_Send(&somma, 1, MPI_INT, tag, tag, MPI_COMM_WORLD);
                }
            }
        }
    }
    else if(strategia == 3){
        for (i = 0; i < log2(n_processi); i++) {
            int mod = pow(2,i+1);
            if (pid % mod < pow(2,i)) {
                tag = pid;
                MPI_Send(&somma, 1, MPI_INT, pid+pow(2,i), tag, MPI_COMM_WORLD);
                MPI_Recv(&somma_parziale, 1, MPI_INT, pid+pow(2,i), tag, MPI_COMM_WORLD, &status);
            } else {
                tag = pid - pow(2,i);
                MPI_Send(&somma, 1, MPI_INT, pid-pow(2,i), tag, MPI_COMM_WORLD);
                MPI_Recv(&somma_parziale, 1, MPI_INT, pid-pow(2,i), tag, MPI_COMM_WORLD, &status);
            }
            somma=somma+somma_parziale; //il processo somma ad ogni passo temporale
        }
    }
\end{lstlisting}

\subsection{Presa tempo totale}

\begin{lstlisting}
    t_1=MPI_Wtime();//tutti i processi prendono il tempo finale.

    time= t_1 - t_0;//ogni processo calcola il proprio tempo.
    printf("sono il processo %d, tempo impiegato:%e\n",pid,time);
    
    MPI_Reduce(&time,&tempo_tot,1,MPI_DOUBLE,MPI_MAX,0,MPI_COMM_WORLD);  // utilizziamo la reduce sui vari tempi per ottenere il massimo (contenuto dal processo 0).
\end{lstlisting}

\subsection{Stampa dei risultati}

\begin{lstlisting}
    //Stampo la somma e il tempo totale di esecuzione con il processo con pid 0.
    if(pid==0){
	FILE *fp;
	fp=fopen("/homes/DMA/PDC/2024/<LOGIN>/somma/tempi.txt","a");
	fprintf(fp,"dimensione=%d processori=%d strategia=%d\n",dim,n_processi,strategia);
        fprintf(fp,"somma=%d\n",somma);
        fprintf(fp,"tempo totale di esecuzione--->%e\n\n\n",tempo_tot);
	fclose(fp);
    }

    //Fine delle istruzioni MPI
    MPI_Finalize();

}

void gestisciErrore(int error_code)
{   
    FILE* fp = fopen("/homes/DMA/PDC/2024/<login>/somma/somma.err","a")
    if (error_code == BAD_STRATEGY_VALUE) {
        //Stampo nel file di errori l'errore trovato.
        fprintf(fp, "Errore nell'inserimento di 'strategia'.\n");
    }
    else
    if (error_code == BAD_ARRAY_SIZE) {
        fprintf(fp, "Errore nell'inserimento di 'dim'.\n");
    }
    else
    if (error_code == ERRORE_ALLOCAZIONE_MEMORIA) {
        fprintf(fp, "Errore di allocazione in memoria.\n");
    }
}

int isPowerOf2(int x)
{
    return (x != 0) && ((x & (x - 1)) == 0);
}
\end{lstlisting}


\section{Analisi dei tempi di esecuzione}
\subsection{Tempo di esecuzione}

Nel calcolare il tempo di esecuzione di un algoritmo parallelo, la grandezza nota come complessità di tempo (utile nell'analisi del tempo di esecuzione di un algoritmo sequenziale) risulta poco indicativa.\\
Infatti, in un algoritmo parallelo il numero delle operazioni non coincide più con il numero dei passi temporali. Di conseguenza si introducono nuove grandezze al fine di realizzare un'analisi degli algoritmi paralleli.

In un ambiente di calcolo parallelo con P che indica il numero di processori, un problema che si risolve in un tempo T verrà risolto da P processori (idealmente) in $\frac{T}{P}$.


\subsection{Speed-up, Overhead ed Efficienza}

Lo \textbf{speed-up} misura la riduzione del tempo di esecuzione rispetto all'algoritmo sequenziale ed è definito dal rapporto:
$$ S(P) = \frac{T(1)}{T(P)} $$ 


La quantità che misura quanto il nostro speed up differisce da quello ideale è l'overhead. Si può quantificare con la seguente formula: $O_h(p) = pT(p) - T(1)$ .\\
Riscrivendo lo speed up in funzione dell'overhead ci accogiamo infatti che lo speed up è ideale se e solo se l'overhead è nullo.\\

Per poter misurare se e quanto è stato "sfruttato" il nostro ambiente di calcolo parallelo, si introduce l' \underline{efficienza} del nostro algoritmo.
Si definisce \textbf{efficienza} il rapporto: $$ E(p) = \frac{S(p)}{p} $$

\subsection{Speed-up scalato e isoefficienza}
Sebbene abbiamo ora delle grandezze per misurare i tempi di esecuzione di un algoritmo parallelo (speed up ed efficienza),
ci chiediamo se sia possibile ottenere speed-up prossimi a quello ideale.\\
Dobbiamo a tal fine inserire il concetto di \textbf{scalabilità}.
Tutto parte dal concetto di dividere un algoritmo in due parti:
\begin{itemize}
    \item $\alpha$ è la parte che può essere eseguita esclusivamente in maniera sequenziale.
    \item $1-\alpha$ è la parte che può essere parallelizzata fra più processori.
\end{itemize}
Individuando queste due componenti principali possiamo arrivare a formulare un nuovo modello di speed up:

$$S(p)=\frac{1}{\alpha-\frac{(1-\alpha)}{p}}$$

Questo modello, noto come \textbf{legge di Ware}, ci permette di osservare l'andamento dello speed-up al variare della dimensione del problema e del numero di processori.
La soluzione ottimale per ottenere uno speed up vicino a quello ideale e, in contemporanea, un'efficienza che \underline{non degradi}, dobbiamo far variare in contemporanea sia il numero dei processori che la dimensione del problema.
Arriviamo così al concetto di speed up scalato:
$$SS(p,n)=\frac{T(1,pn)}{T(p,pn)}=\frac{pT(1,n)}{T(p,pn)}$$

Ma di quanto deve aumentare la dimensione del problema in proporzione al numero dei processori?\\
Introduciamo, per questo motivo, una funzione che calcola esattamente questo coefficiente, la funzione isoefficienza, che per il nostro algoritmo risulta: 
$$I(n_{0}, p_{0}, p_{1}) = n_0\frac{p_{1} log_{2} p_{1}}{p_{0} log_{2} p_{0}} = n_1$$
Osserviamo inoltre che:
\begin{enumerate}
    \item Se aumentiamo p e teniamo n fissato allora le prestazioni peggiorano!.
    \item Se aumentiamo n e teniamo p fissato allora lo speed up si avvicina sempre di piu a quello ideale (ma non possiamo aumentare arbitrariamente la dimensione del problema con un hardware limitato...).
    \item Se aumentiamo n proporzionalmente a p secondo un coefficiente ricavato dall isoefficienza, allora otteniamo speed-up vicino a quello ideale e l'efficienza rimane "costante". (migliore soluzione)
\end{enumerate}
Grazie a questi nuovi concetti possiamo finalmente tentare di ottenere speed up (scalati) tendenti allo speed up ideale.
\subsection{Dati empirici}
L'algoritmo è stato testato in più condizioni e secondo i parametri precedentemente descritti.

Avendo fissato ad un milione la dimensione del problema ed avendo fatto variare il numero di processori da 2 a 8 (per il caso di singolo processore, si è deciso di eseguire l'algoritmo sequenziale per la somma), tenendo conto esclusivamente delle potenze di 2, sono stati ottenuti i seguenti risultati:

\begin{table}[htp]
\centering
\caption{Strategia 1}
\begin{tblr}{
  hlines,
  vlines,
}
Processori & Tempo ($\cdot 10^{-3} s$) & Speed Up & Efficienza & Overhead ($\cdot 10^{-3} s$) \\
2          & 2.10   & 1.84    & 0.92       & 0.34                     \\
4          & 1.12   & 3.45    & 0.86       & 0.62                     \\
8          & 0.61   & 6.3     & 0.79       & 1.04                     
\end{tblr}
\end{table}

\begin{table}[htp]
\centering
\caption{Strategia 2}
\begin{tblr}{
  hlines,
  vlines,
}
Processori & Tempo ($\cdot 10^{-3} s$) & Speed Up & Efficienza & Overhead ($\cdot 10^{-3} s$) \\
2          & 2.06 & 1.87     & 0.93       & 0.26                     \\
4          & 1.08 & 3.57     & 0.86       & 0.46                     \\
8          & 0.60 & 6.41     & 0.80       & 0.94                     
\end{tblr} 
\end{table}

\begin{table}[h!tbp]
\centering
\caption{Strategia 3}
\begin{tblr}{
  hlines,
  vlines,
}
Processori & Tempo ($\cdot 10^{-3} s$) & Speed Up & Efficienza & Overhead ($\cdot 10^{-3} s$) \\
2          & 2.29 & 1.68     & 0.84       & 0.72                     \\
4          & 1.18 & 3.27     & 0.82       & 0.86                     \\
8          & 0.63 & 6.10     & 0.76       & 1.18                     
\end{tblr}
\end{table}

\clearpage

Il grafico dei tempi:
\begin{figure}[h!tbp]
    \centering
    \includegraphics[width=1\linewidth]{comparisonTempi.png}
    \caption{Andamento dei tempi}
    \label{fig:enter-label}
\end{figure}

\newpage
Visualizziamo la curva, confrontandola all'andamento dello speed-up ideale:
\begin{figure}[h!tbp]
    \centering
    \includegraphics[width=1\linewidth]{comparisonSpeedUp.png}
    \caption{Andamento degli Speed Up}
    \label{fig:enter-label}
\end{figure}
\clearpage
Visualizziamo su un grafico anche i valori degli Overhead ottenuti per ogni strategia:

\begin{figure}[h!tbp]
    \centering
    \includegraphics[width=1\linewidth]{comparisonOverhead.png}
    \caption{Andamento degli Overhead}
    \label{fig:enter-label}
\end{figure}

\clearpage

Infine osserviamo come varia la nostra efficienza:
\begin{figure}[h!tbp]
    \centering
    \includegraphics[width=1\linewidth]{comparisonEfficienza.png}
    \caption{Andamento dell' efficienza}
    \label{fig:enter-label}
\end{figure}

\section{Modalità d'uso}
Il programma deve essere eseguito mediante un job script contenente direttive pbs (portable batch system).

Un esempio di jobscript:
\begin{lstlisting}
#!/bin/bash

#Sostituire NUMERO_DI_PROCESSORI e FILE in ogni istanza

#PBS -q studenti
#PBS -l nodes= NUMERO_DI_PROCESSORI
#PBS -N FILE
#PBS -o FILE.out
#PBS -e FILE.err

# -q coda su cui va eseguito il job
# -l numero di nodi richiesti
# -N nome job(stesso del file pbs)
# -o, -e nome files contenente l’output

NCPU=`wc -l < $PBS_NODEFILE`
echo ----------------------------
echo 'This job is allocated on '${NCPU}' cpu(s)'
echo 'Job is running on node(s): '
cat $PBS_NODEFILE

PBS_O_WORKDIR=$PBS_O_HOME/DIRECTORY_OF_FILE

echo "Compilo..."
/usr/lib64/openmpi/1.4-gcc/bin/mpicc -o $PBS_O_WORKDIR/FILE $PBS_O_WORKDIR/FILE.c

echo "Eseguo..."
/usr/lib64/openmpi/1.4-gcc/bin/mpiexec -machinefile $PBS_NODEFILE -np $NCPU $PBS_O_WORKDIR/FILE
\end{lstlisting}

Una volta scritto il job script, per eseguirlo, bisognerà eseguire da terminale il comando 
\begin{lstlisting}
qsub FILE.pbs
\end{lstlisting}

In caso l'esecuzione dello script vada a buon fine, verranno generati FILE.out e FILE.err, i file di output e errore rispettivamente.

Per andare a visualizzare tali file, bisognerà eseguire da terminale i comandi
\begin{lstlisting}
cat FILE.err
cat FILE.out
\end{lstlisting}

\end{document}
